<h1>Awesome-LLM-VLM-Foundation-Models üöÄ‚≠ê‚≠ê‚≠ê</h1> 
<p>Awesome curated list of LLM, VLM and other Foundation Models</p>

<div style="overflow-x:auto;">
<table>
  <thead>
    <tr>
      <th>No.</th>
      <th>Model</th>
      <th>Year</th>
      <th>Company</th>
      <th>Size & Context Window</th>
      <th>Best For / Strengths</th>
      <th>Access & Cost</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>1</td><td>GPT‚Äë4.5 ‚ÄúOrion‚Äù</td><td>2025-02</td><td>OpenAI</td><td>~12‚ÄØT params / 128K tokens</td><td>Nuanced conversation, creative writing, lower hallucinations</td><td>ChatGPT Pro/Premium $200‚ÄØ/mo; API: $75/1M in,‚ÄØ$150/1M out</td></tr>
    <tr><td>2</td><td>GPT‚Äë4.1</td><td>2025-04</td><td>OpenAI</td><td>Large / 1M tokens</td><td>Long-context docs, coding, vision, low latency</td><td>API: input $2, output $8 per 1M tokens; Mini variant cheaper</td></tr>
    <tr><td>3</td><td>GPT‚Äë4o (‚ÄúOmni‚Äù)</td><td>2024-05</td><td>OpenAI</td><td>Multimodal / 128K tokens</td><td>Text+image+audio+voice; fast & free-tier use</td><td>Free-tier in ChatGPT; API: $2.50/1M in, $10/1M out</td></tr>
    <tr><td>4</td><td>GPT‚Äë4o mini</td><td>2024-07</td><td>OpenAI</td><td>~8‚ÄØB params / 128K tokens</td><td>Cost-effective multimodal</td><td>ChatGPT replacement; API: $0.15/1M in, $0.60/1M out</td></tr>
    <tr><td>5</td><td>o4‚Äëmini‚Äëhigh / o4‚Äëmini</td><td>2025-04</td><td>OpenAI</td><td>Compact reasoning / multimodal</td><td>STEM, coding, fast reasoning with vision</td><td>API: input $1.10, output $4.40 per 1M tokens</td></tr>
    <tr><td>6</td><td>o3‚Äëmini‚Äëhigh / o3‚Äëmini</td><td>2024</td><td>OpenAI</td><td>Small reasoning models</td><td>Technical/scientific reasoning on a budget</td><td>API: same pricing as o3 & mini models</td></tr>
    <tr><td>7</td><td>Llama 4 Maverick</td><td>2025</td><td>Meta AI</td><td>Large Mixture‚Äëof‚ÄëExpert (128 experts), 400B parameters, 1M context</td><td>Coding, reasoning; GPT‚Äë4o‚Äëlevel</td><td>$0.19-$0.49/1M in & out tokens</td></tr>
    <tr><td>8</td><td>Llama 4 Scout</td><td>2025</td><td>Meta AI</td><td>Small (fits 1 A100/H100) / 109B parameters, 10M context</td><td>Generalist, long context small model</td><td>Open weights</td></tr>
    <tr><td>9</td><td>Llama 3.1 405B</td><td>2024</td><td>Meta AI</td><td>405 B params / 128K tokens</td><td>Research, long-context, coding</td><td>Open source</td></tr>
    <tr><td>10</td><td>Claude 3.7 Sonnet</td><td>2024-10</td><td>Anthropic</td><td>~175 B params / 200K tokens</td><td>Extended reasoning & coding</td><td>API (paid via Anthropic)</td></tr>
    <tr><td>11</td><td>Gemini 2.5 Pro</td><td>2024-05</td><td>Google DM</td><td>Undisclosed; multimodal / 1M tokens</td><td>Advanced reasoning, multimodal</td><td>API (paid via Google)</td></tr>
    <tr><td>12</td><td>Stable LM 2 12B</td><td>2024-04</td><td>Stability AI</td><td>12 B params</td><td>Open model with good benchmarks</td><td>Open source</td></tr>
    <tr><td>13</td><td>Qwen 2.5-VL 32B</td><td>2025-03</td><td>Alibaba</td><td>32 B params; multimodal / 128K tokens</td><td>Vision+language tasks</td><td>Open source (Apache 2.0)</td></tr>
    <tr><td>14</td><td>Mistral Small 3.1</td><td>2025-03</td><td>Mistral AI</td><td>24 B params; 128K tokens</td><td>Image & doc understanding</td><td>Open source (Apache 2.0)</td></tr>
    <tr><td>15</td><td>Gemma 3 (27B)</td><td>2025-03</td><td>Google DM</td><td>27 B params</td><td>One-GPU efficient model</td><td>Open source</td></tr>
    <tr><td>16</td><td>Fox-1 1.6B Instruct</td><td>2024-11</td><td>Fox-1 project</td><td>1.6 B params</td><td>Instruction-following small LLM, conversational</td><td>Open source (Apache 2.0)</td></tr>
    <tr><td>17</td><td>Grok 3</td><td>2025-02</td><td>xAI (Elon Musk)</td><td>Unknown (Chat-focused)</td><td>Conversational AI, Twitter/X integration</td><td>Proprietary (likely X Premium)</td></tr>
    <tr><td>18</td><td>DeepSeek R1</td><td>2025</td><td>DeepSeek AI</td><td>Reasoning-focused</td><td>Reasoning tasks, competitive with GPT‚Äë4.5</td><td>Open weights</td></tr>
    <tr><td>19</td><td>Cerebras Qwen3-32B</td><td>2025-05</td><td>Cerebras</td><td>32 B params</td><td>High-speed reasoning</td><td>Open source (Apache 2.0)</td></tr>
    <tr><td>20</td><td>Grok 4</td><td>2025-07</td><td>xAI (Elon Musk)</td><td>~2.4T params / 260K tokens</td><td>Advanced reasoning, coding (Grok 4 Code), multimodal, real-time data</td><td>SuperGrok $30/mo, Heavy $300/mo; API: $3/1M in, $15/1M out; X Premium+ access</td></tr>
    <tr><td>21</td><td><a href="https://github.com/MoonshotAI/Kimi-K2">Kimi K2</a></td><td>2025-07</td><td>Moonshot AI</td><td>1T params (32B active) / 128K tokens</td><td>Mixture-of-experts (MoE). Agentic intelligence, coding, reasoning, tool use</td><td>Open source (Modified MIT); API: $0.15/1M in, $2.50/1M out</td></tr>
    <tr><td>22</td><td>gpt-oss-20b</td><td>2025-08</td><td>OpenAI</td><td>21B params (3.6B active) / 128K tokens</td><td>Reasoning, agentic tasks, local deployment, low latency</td><td>Open source (Apache 2.0), downloadable via Hugging Face, Ollama, GitHub</td></tr>
    <tr><td>23</td><td>gpt-oss-120b</td><td>2025-08</td><td>OpenAI</td><td>117B params (5.1B active) / 128K tokens</td><td>Deep reasoning, agentic tasks, enterprise-grade deployment</td><td>Open source (Apache 2.0), downloadable via Hugging Face, Ollama, GitHub</td></tr>
    <tr><td>24</td><td>GPT‚Äë5</td><td>2025-08</td><td>OpenAI</td><td>~15T params / 400K tokens</td><td>Advanced reasoning, coding, multimodal, scientific tasks</td><td><a href="https://openai.com/api/pricing">API:</a> $1.25/1M in, $10/1M out; ChatGPT Plus/Pro/Team, Free-tier access</td></tr>

  </tbody>
</table>
</div>


## Foundation Models Leaderboards (2025)

This is a curated list of new and up-to-date leaderboards for Large Language Models (LLMs), Vision-Language Models (VLMs), and multimodal models, published or updated in 2025. Each leaderboard provides performance metrics, rankings, and comparisons for state-of-the-art foundation models.

1. **[LLM Leaderboard 2025 - llm-stats.com](https://llm-stats.com)**  
   Comprehensive leaderboard for LLMs with performance metrics and benchmark data. Includes interactive analysis tools to compare models like GPT-4o, Llama, o1, Gemini, and Claude based on context window, speed, and price. 

2. **[Open LLM Leaderboard - Hugging Face](https://huggingface.co/open-llm-leaderboard)**  
   Evaluates open-source LLMs using benchmarks like IFEval, BBH, and MATH. Features real-time filtering and analysis of models, with community voting and comprehensive results.

3. **[LLM Leaderboard 2025 - Vellum](https://www.vellum.ai/llm-leaderboard)**  
   Compares capabilities, price, and context window for leading commercial and open-source LLMs. Features 2025 benchmark data from model providers and independent evaluations, focusing on non-saturated benchmarks (excluding MMLU). 

4. **[LLM Leaderboard - Artificial Analysis](https://artificialanalysis.ai)**  
   Ranks over 100 LLMs across metrics like intelligence, price, performance, speed (tokens per second), and context window. Provides detailed comparisons for models from OpenAI, Google, DeepSeek, Alibaba Cloud and others. 

5. **[SEAL LLM Leaderboards](https://scale.com/leaderboards)**  
   Expert-driven, private evaluations of LLMs across domains like coding and instruction following. Uses curated datasets to prevent overfitting and ensure high-complexity evaluations. 

6. **[Open VLM Leaderboard - Hugging Face](https://huggingface.co/spaces/opencompass/open_vlm_leaderboard)**  
   Ranks open-source VLMs using 23 multimodal benchmarks (e.g., MMBench_V11, MathVista). Evaluates models like GPT-4v, Gemini, QwenVLPlus, and LLaVA on image-text tasks.

7. **[Zero-Shot Video Question Answer on Video-MME](https://paperswithcode.com/sota/zero-shot-video-question-answer-on-video-mme-1)**  
   This task present the results of Zeroshot Question Answer results on TGIF-QA dataset for LLM powered Video Conversational Models.

## Frameworks and Tools for LLMs, VLMs, and Foundation Models (2025)

This list highlights key frameworks, tools, and libraries for developing, deploying, and managing Large Language Models (LLMs), Vision-Language Models (VLMs), and foundation models.

## üõ† Application Development & Prompt Engineering Frameworks

1. **[LangChain](https://www.langchain.com)**  
   A versatile framework for building LLM-powered applications. It simplifies prompt chaining, memory management, and integration with external data sources like vector databases and APIs. Used for chatbots, RAG systems, and agent-based workflows.

2. **[LlamaIndex](https://www.llamaindex.ai)**  
   A data framework designed for connecting LLMs with custom data sources. It excels in data ingestion, indexing, and retrieval for RAG applications, enabling semantic search and context-aware querying. Ideal for document analysis and knowledge base systems.

3. **[DSPy](https://dspy.ai)**  
   A framework for programming foundation models by defining tasks rather than crafting prompts. It optimizes pipelines for LLMs using modular components, improving performance in tasks like reasoning and text generation. Suited for developers seeking maintainable codebases.

4. **[Semantic Kernel](https://devblogs.microsoft.com/semantic-kernel)**  
   A Microsoft-developed SDK for integrating LLMs into applications. It supports orchestration of AI tasks, memory management, and plugins for connecting to external tools. Used for building scalable AI agents in Python, C#, and Java.

5. **[AutoGen](https://microsoft.github.io/autogen)**  
   A Python-based framework for creating multi-agent LLM systems. It enables agents to collaborate on tasks like data retrieval and code execution, enhancing complex workflows. Used for building autonomous AI agents and research.

---

## üîç Retrieval-Augmented Generation (RAG) & Semantic Search

1. **[Haystack](https://haystack.deepset.ai)**  
   An open-source framework for building LLM-powered search and RAG applications. It supports semantic search, document retrieval, and question answering, with integrations for Hugging Face, OpenAI, and vector stores like Pinecone. Used for enterprise search systems.

2. **[Chroma](https://github.com/chroma-core/chroma)**  
   An open-source embedding database optimized for managing and searching vector embeddings. Commonly used for semantic search and RAG pipelines with LangChain or LlamaIndex.

3. **[Jina](https://jina.ai)**  
   A scalable cloud-native framework for multimodal search and neural semantic retrieval. Supports building RAG pipelines with images, text, and more.

4. **[Qdrant](https://github.com/qdrant/qdrant)**  
   An open-source vector search engine for storing and querying embeddings at scale. Built for semantic search, recommendation engines, and RAG applications.

---

## üöÄ Model Serving & Deployment

1. **[Ollama](https://ollama.com)**  
    A lightweight framework for running LLMs locally. It provides a simple API and supports models like Llama 3 and Gemma, enabling developers to build and test AI applications on personal hardware. Perfect for local AI development and prototyping.

2. **[OpenLLM](https://github.com/bentoml/OpenLLM)**  
    Run any open-source LLMs (Llama 3.3, Qwen2.5, Phi3 and more) or custom models as OpenAI-compatible APIs with a single command.

3. **[vLLM](https://github.com/vllm-project/vllm)**  
    An open-source library designed to serve LLMs efficiently and at scale, especially for inference. Uses PagedAttention to optimize memory usage, batching, and throughput.

4. **[Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference)**  
    Hugging Face‚Äôs optimized inference server for deploying large Transformer models with low latency and high throughput.

5. **[FastChat](https://github.com/lm-sys/FastChat)**  
    A powerful open-source framework to serve and chat with LLMs interactively. Includes a web UI, REST API, and support for various model families like Vicuna and LLaMA.

---

## ‚öôÔ∏è ML Workflow Automation & Management

1. **[MLflow](https://mlflow.org)**  
    An open-source platform for managing the machine learning lifecycle, including LLMs and VLMs. It supports experiment tracking, model versioning, and deployment, with integrations for LangChain, LlamaIndex, and DSPy. Ideal for reproducible AI workflows.

2. **[n8n](https://n8n.io)**  
    An open-source, low-code workflow automation platform. It integrates LLMs with external tools and APIs to automate tasks like data processing or chatbot responses. Used for building scalable AI-driven workflows with minimal coding.

3. **[Flowise](https://flowiseai.com)**  
    An open-source, low-code platform for building LLM applications. It features a drag-and-drop interface and integrates with LangChain and LlamaIndex, making it accessible for non-coders to create chatbots and RAG systems.

---

## üßë‚Äçüîß Fine-Tuning & Training Optimization

1. **[Hugging Face Transformers](https://huggingface.co/docs/transformers/installation)**  
    A comprehensive library for training, fine-tuning, and deploying LLMs and VLMs. It supports models like BERT, GPT, and CLIP, with tools for NLP, computer vision, and multimodal tasks. Used for research and production-grade AI applications.

2. **[PEFT (Parameter-Efficient Fine-Tuning)](https://github.com/huggingface/peft)**  
    A library for efficient fine-tuning of large models using techniques like LoRA, prompt tuning, and adapters. Ideal for customizing LLMs on limited hardware.

3. **[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)**  
    A lightweight CUDA extension for quantization and low-bit inference/training of LLMs. Enables memory-efficient training of large models.

4. **[LMFlow](https://github.com/OptimalScale/LMFlow)**  
    A framework for easy and fast fine-tuning, instruction tuning, and deployment of LLMs. Includes support for model compression and evaluation.

---

## ‚úÖ Evaluation, Testing & Benchmarking

1. **[DeepEval](https://www.deepeval.com)**  
    A testing framework for evaluating LLM applications. It offers over 14 research-backed metrics to assess RAG pipelines and safety risks, integrating with frameworks like LangChain and LlamaIndex. Used for quality assurance in AI development.

2. **[PromptTools](https://github.com/hegelai/prompttools)**  
    A Python library for debugging, comparing, and evaluating LLM prompts with visualizations and logging support.

3. **[AlpacaEval](https://github.com/tatsu-lab/alpaca_eval)**  
    A community-driven evaluation toolkit for benchmarking LLMs' instruction-following ability using standardized prompts.

4. **[OpenCompass](https://github.com/open-compass/opencompass)**  
    A comprehensive open-source framework for large-scale benchmarking of LLMs and VLMs using curated datasets and metrics.

5. **[OpenCompass](https://github.com/open-compass/opencompass)**  
    A comprehensive open-source framework for large-scale benchmarking of LLMs and VLMs using curated datasets and metrics.

6. **[VLMEvalKit](https://github.com/open-compass/VLMEvalKit)** üñºÔ∏è  
    Open-source evaluation toolkit of large vision-language models (LVLMs). It enables one-command evaluation of LVLMs on various benchmarks (support 220+ LMMs, 80+ benchmarks).

---

## üìä Interactive UI & Demos

1. **[Gradio](https://www.gradio.app)**  
    An intuitive Python library for creating interactive web interfaces for ML models. Popular for prototyping and demonstrating LLM/VLM applications.

2. **[Open WebUI](https://github.com/open-webui/open-webui)**  
    An open-source web interface for interacting with local and hosted LLMs. Supports multiple backends and provides a sleek, extensible UI.

---

## üé® Multimodal & Vision-Language Models

1. **[OpenMMLab Multimoda-GPTl](https://github.com/open-mmlab/Multimodal-GPT)**  
    Based on the open-source multi-modal model OpenFlamingo create various visual instruction data with open datasets, including VQA, Image Captioning, Visual Reasoning, Text OCR, and Visual Dialogue.
    
2. **[OpenMMLab MMagic](https://github.com/open-mmlab/mmagic)**  
    Multimodal Advanced, Generative, and Intelligent Creation (MMagic).

---

## üß† Interpretability & Analysis

1. **[Transformer Lens](https://github.com/neelnanda-io/TransformerLens)**  
    A library for visualizing and interpreting transformer internals. Helps researchers understand model behavior neuron-by-neuron.

---

## üñºÔ∏è Vision‚ÄëLanguage Model (VLM) Zoo

1. **[GLM-4.1V-9B-Thinking](https://github.com/THUDM/GLM-4.1V-Thinking), [ü§ó HF](https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking) - MIT License üöÄ**  
  Open-source VLM from THUDM, excelling in multimodal reasoning with support for 64k context, 4K image processing, and bilingual (English/Chinese) capabilities. It outperforms many models of similar size and rivals larger models like Qwen2.5-VL-72B on 18/28 benchmarks, including STEM and long document understanding.
   
2. **[Qwen‚ÄØ2.5‚ÄØVL (7B / 72B)](https://github.com/QwenLM/Qwen)**  
   Multimodal VLM from Alibaba with dynamic resolution, video input, object localization and support for ~29 languages. Top open‚Äësource performer in OCR and agentic workflows.

3. **[Gemma‚ÄØ3 (4B‚Äì27B)](https://deepmind.google/models/gemma/gemma-3)**  
   Google‚Äôs open multimodal model with SigLIP image encoder, excels in multilingual captioning and VQA; strong 128k context performance.

4. **[PaliGemma](https://huggingface.co/blog/paligemma)**  
   Compact G·¥á·¥ç·¥ç·¥Ä‚Äë2‚ÄØB‚Äëbased VLM combining SigLIP visual encoder with strong captioning, segmentation, and VQA transferability.

5. **[Llama‚ÄØ3.2‚ÄØVision (11B/90B)](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices)**  
   Vision‚Äëadapted Llama model with excellent OCR, document understanding, VQA, and 128k token context.

6. **[Phi‚Äë4‚ÄØMultimodal](https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090)**  
   Microsoft‚Äôs VLM supporting vision‚Äëlanguage tasks with MIT license and edge‚Äëfriendly capabilities.

7. **[DeepSeek‚ÄëVL](https://github.com/deepseek-ai/DeepSeek-VL)**  
   Open‚Äësource VLM optimized for scientific reasoning and compact deployment.

8. **[CogVLM](https://github.com/THUDM/CogVLM)**  
   Strong-performing model in VQA and vision-centric tasks.

9. **[BakLLaVA](https://github.com/SkunkworksAI/BakLLaVA)**  
   LAION‚ÄëOntocord-Skunkworks OSS AI group LMM combining Mistral‚ÄØ7B with LLaVA architecture for efficient VQA pipelines.

## üìÑ OCR Model Zoo

1. **[OCRFlux](https://github.com/chatdoc-com/OCRFlux)**  
   OCRFlux is a multimodal large language model based toolkit for converting PDFs and images into clean, readable, plain Markdown text. A 3B parameter model that can run on a single NVIDIA 3090 GPU, making it accessible for local deployment.
   
2. **[Llama-3.1-Nemotron-Nano-VL-8B-V1](https://build.nvidia.com/nvidia/llama-3.1-nemotron-nano-vl-8b-v1/modelcard),  [ü§ó HF](https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1)**  
  Llama-Nemotron-Nano-VL-8B-V1 (by NVIDIA) is a leading document intelligence vision language model (VLMs) that enables the ability to query and summarize images and video from the physical or virtual world.
   
3. **[Qwen‚ÄØ2.5‚ÄØVL (32B / 72B)](https://github.com/QwenLM/Qwen)**  
   State‚Äëof‚Äëthe‚Äëart open OCR performance (~75% accuracy), outperforms even Mistral‚ÄëOCR; excels in document, video, and multilingual text extraction.
   
4. **[Mistral‚ÄëOCR](https://mistral.ai/news/mistral-ocr)**  
   Purpose‚Äëtrained OCR variant of Mistral, delivering ~72.2% accuracy on structured document benchmarks.

5. **[Llama‚ÄØ3.2‚ÄØVision (11B / 90B)](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices)**  
   Strong OCR and document understanding capabilities, part of the top open VLMs.

6. **[Gemma‚ÄØ3‚ÄØ27B](https://deepmind.google/models/gemma/gemma-3)**  
   Offers competitive OCR performance through its vision‚Äëlanguage architecture.

7. **[DeepSeek‚Äëv3‚Äë03‚Äë24](https://github.com/deepseek-ai/DeepSeek-V3)**  
   Lightweight, open‚Äësource OCR-ready VLM evaluated in 2025 benchmarks.
   
8. **[TextHawk‚ÄØ2](https://github.com/yuyq96/TextHawk)**  
   Bilingual OCR and grounding VLM showing state‚Äëof‚Äëthe‚Äëart across OCRBench, DocVQA, ChartQA, with 16√ó fewer tokens.

9. **[VISTA‚ÄëOCR](https://arxiv.org/abs/2504.03621)**  
   New lightweight generative OCR model unifying detection and recognition with only 150M params; interactive and high‚Äëaccuracy.

10. **[PP‚ÄëDocBee](https://github.com/PaddlePaddle/PaddleMIX)**  
   Multimodal document understanding model with superior performance on English/Chinese benchmarks.

## üìÑ Medical LLMs, VLMs and MLLMs (multimodal)

| Model                                                                 | Year       | Company/Org             | Used for                                                                                          |
|------------------------------------------------------------------------|------------|------------------------------|---------------------------------------------------------------------------------------------------|
| **[MedGemma](https://deepmind.google/models/gemma/medgemma)**              | 2025       | Google DeepMind              | OCR, image captioning, general vision NLP (3B‚Äì28B parameters)                                     |
| **[MedSigLIP](https://github.com/google-health/medsiglip)** | 2025       | Google DeepMind              | Scalable multimodal medical reasoning                                  |
| **[Med‚ÄëGemini](https://research.google/blog/advancing-medical-ai-with-med-gemini)** | 2024   | Google DeepMind              | Multimodal medical applications                                           |
| **[LLaVA-Med](https://github.com/microsoft/LLaVA-Med)** | 2024   | Microsoft              | Large Language-and-Vision Assistant for Biomedicine, built towards multimodal GPT-4 level capabilities                                           |
| **[CONCH](https://github.com/mahmoodlab/CONCH)** | 2024   | Mahmood Lab + Harvard Medical School               | Vision-Language Pathology Foundation Model - Nature Medicine                                           |
| **[BioMistral‚Äë7B](https://huggingface.co/BioMistral/BioMistral-7B)**      | 2024       | CNRS + Mistral               | Medical-domain fine-tuned LLM on PubMed (7B parameters)                                           |
| **[BioMedLM 2.7B](https://huggingface.co/stanford-crfm/BioMedLM)**      | 2024       | Stanford CRFM+MosaicML              | Medical-domain  trained exclusively on biomedical abstracts and papers from The Pile                                           |
| **[Med‚ÄëPaLM M](https://sites.research.google/med-palm)**               | 2022-2023       | Google Research              | Multimodal medical Q&A with image and text input                      |



# Papers üìë ‚≠ê
| No. | Title                                                                                | Authors                      | Journal Name                                         | Year |
| --- | ------------------------------------------------------------------------------------ | ---------------------------- | ---------------------------------------------------- | ---- |
| 1   | Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models     | Yukang Yang, et al.          | [arXiv:2502.20332](https://arxiv.org/abs/2502.20332) | 2025 |
| 2   | Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models               | James Chua, et al.           | [arXiv:2506.13206](https://arxiv.org/abs/2506.13206) | 2025 |
| 3   | Emergent Response Planning in Large Language Models                                  | Zhichen Dong, et al.         | [arXiv:2502.06258](https://arxiv.org/abs/2502.06258) | 2025 |
| 4   | Emergent Abilities in Large Language Models: A Survey                                | Leonardo Berti, et al.       | [arXiv:2503.05788](https://arxiv.org/abs/2503.05788) | 2025 |
| 5   | LIMO: Less Is More for Reasoning                                                     | Yixin Ye, et al.             | [arXiv:2502.03387](https://arxiv.org/abs/2502.03387) | 2025 |
| 6   | An Introduction to Vision-Language Modeling                                          | Florian Bordes, et al.       | [arXiv:2405.17247](https://arxiv.org/abs/2405.17247) | 2024 |
| 7   | What Matters When Building Vision-Language Models?                                   | Hugo Lauren√ßon, et al.       | [arXiv:2405.02246](https://arxiv.org/abs/2405.02246) | 2024 |
| 8   | Building and better understanding vision-language models: insights and future directions           | Hugo Lauren√ßon, et al.       | [arXiv:2408.12637](https://arxiv.org/abs/2408.12637) | 2024 |
| 9   | DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding | Zhiyu Wu, et al.             | [arXiv:2412.10302](https://arxiv.org/abs/2412.10302) | 2024 |
| 10  | Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution    | Peng Wang, et al.            | [arXiv:2409.12191](https://arxiv.org/abs/2409.12191) | 2024 |
| 11  | PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter                        | Junfei Xiao, et al.          | [arXiv:2402.10896](https://arxiv.org/abs/2402.10896) | 2024 |
| 12  | Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving       | Akshay Gopalkrishnan, et al. | [arXiv:2403.19838](https://arxiv.org/abs/2403.19838) | 2024 |

# Conferences & Papers ü•á üìë ‚≠ê
1. **[CVPR](https://cvpr.thecvf.com) - IEEE / CVF Computer Vision and Pattern Recognition Conference**
   * [2025 Accepted Papers](https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers)
   * [2025 Posters](https://cvpr2025.vizhub.ai)  
    
2. **[NeurIPS](https://neurips.cc) - Conference on Neural Information Processing Systems**
   
3. **[ICLR](https://iclr.cc/) - International Conference on Learning Representations**
   * [2025 Accepted Papers](https://iclr.cc/virtual/2025/papers.html?filter=topic&search=)
   * [2025 Posters](https://iclr2025.vizhub.ai/)
  
4. **[ACL](https://aclanthology.org/venues/acl/) - Association for Computational Linguistics**

If you need support with your AI project or if you're simply AI and new technology enthusiast, don't hesitate to connect with me on [LinkedIn](https://www.linkedin.com/in/adam-srebro-phd-90a3504b) üëç
  

